{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb70680-ffd7-4a24-96a5-3dbc83e41ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744b9758-ce3d-4ca1-beae-74605718adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "953233a3-ced8-4766-a424-e23fe0116646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec67e36d-fe6b-4654-b5ae-de72c46b92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e20aa6-a4c3-4327-aa0d-7b8795958855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d29f99-a971-4977-9d6f-aeeac91b2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d8adf3-4c29-453c-89a1-5f9371a5ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(data):\n",
    "    result = tokenizer(data)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377add9f-024c-49fa-9819-d390f806322c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc77f758-0284-4f5f-8ccd-9ec4b963c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_chunk_dataset(data):\n",
    "    chunk_size = 128\n",
    "    \n",
    "    # drop the last chunk if is smaller than the chunk size\n",
    "    total_length = len(data[\"input_ids\"])\n",
    "\n",
    "    # split the concatenated sentences into chunks using the total length\n",
    "    result = {k: [t[i: i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in data.items()}\n",
    "    \n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ff2caa-f6c4-4713-bcc4-ae3617d6f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fir_df = pd.read_csv(\"13225_13225097_fir.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed9c7e-4cd5-44cf-8e41-af0e5c7a56e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce64931-8a21-42a7-8021-3775a356ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', return_tensors='pt', padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8b4d6-716c-486c-8d2f-37509bb3ff31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed8972a-2ec0-4c31-af19-f52489e82bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokens = fir_df[\"fir_text\"].apply(lambda x: tokenize_function(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb55041a-6e0b-4c27-91ec-fe8e1c77d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = pd.concat(tokens.apply(lambda x : concat_chunk_dataset(x)).tolist()).drop_duplicates(subset=\"input_ids\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d95c1ed8-4069-49b8-8f02-4d9a31128aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2553, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4cfddd6-a462-48c6-98c5-9e32a45292c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fir_ds = Dataset.from_pandas(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb6b18a0-fb5c-4377-bd69-fbd151042395",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm_probability = .15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c0681b2-6aa9-4467-89fa-10c09a891b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fir_ds = Dataset.from_pandas(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0732ca63-f100-4912-b2b0-072821c4ba3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b810f60f-7990-4384-9650-2a574b0b2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "test_size = int(0.2 * train_size)\n",
    "\n",
    "downsampled_dataset = fir_ds.train_test_split(train_size=train_size, test_size=test_size, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adeae650-2e11-4b76-85c9-6c0f15185a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72eaefe2-fe71-46d9-b413-89bfa29a8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we shall insert mask randomly in the sentence\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc5e5c8f-a1ae-4a2f-99a7-1a00a736c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "#     insert_random_mask,\n",
    "#     batched=True, \n",
    "#     remove_columns = downsampled_dataset[\"test\"].column_names)\n",
    "\n",
    "# eval_dataset = eval_dataset.rename_columns({\"masked_input_ids\": \"input_ids\",\n",
    "# \"masked_attention_mask\": \"attention_mask\",\"masked_labels\": \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40ca3426-6100-48e4-8ec3-df899f344d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f24a70-66ac-4cdd-81ee-d44883df5bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a616b07-111e-4550-ba88-9dfd7e84bdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc057110-8ff8-4034-ba5b-e5b73e0a4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"xlm-roberta-base-finetuned-bigger-chunks\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ab8d82c-46ff-4b02-b419-b26b8a14118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "390a25c2-0f09-44ce-890f-6b3294de9d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 48:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.181948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.979801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.879833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=2.3959768880208334, metrics={'train_runtime': 2890.2427, 'train_samples_per_second': 1.038, 'train_steps_per_second': 0.13, 'total_flos': 197909291520000.0, 'train_loss': 2.3959768880208334, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91b99f61-92b8-48cf-b155-4e3a48b8714d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 7.13\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebc9eb3f-68da-4003-8522-18dfc9cbca3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9640165567398071,\n",
       " 'eval_runtime': 31.8601,\n",
       " 'eval_samples_per_second': 6.277,\n",
       " 'eval_steps_per_second': 0.785,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6432ca77-043e-4475-9757-e838ea06f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/fir_bigger_chunks_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b22066-31dd-4e50-83d0-9d7691ef4c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d607a47-a6ea-475e-a053-9d8948cebac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a54aa8ef-c5d0-494d-95e3-c4a8809787cc",
   "metadata": {},
   "source": [
    "### Discrimination Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af4ffc9d-7496-4900-8610-240d331ad7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f0787db-6359-439f-abcc-c8393c605581",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_masks(sentence, targets):\n",
    "        new_sentence = sentence\n",
    "        for target in targets:\n",
    "            new_sentence = re.sub('MASK',target,new_sentence, count=1)\n",
    "        #print(\"FILLED MASK:\", new_sentence)\n",
    "        return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e1ac15a-5860-4804-b3ae-bacb369a3fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Target_Stereotypical'] = df['Target_Stereotypical'].apply(lambda x: x.replace(\"'\", \"\").replace('[', '').replace(']','').split(','))\n",
    "    df['Target_Anti-Stereotypical'] = df['Target_Anti-Stereotypical'].apply(lambda x: x.replace(\"'\", \"\").replace('[', '').replace(']','').split(','))\n",
    "    df['Stereotypical'] = df.apply(lambda x: fill_masks(x['Sentence'],x['Target_Stereotypical']),axis=1)\n",
    "    df['Anti-Stereotypical'] = df.apply(lambda x: fill_masks(x['Sentence'],x['Target_Anti-Stereotypical']),axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_aul(model, sentence, log_softmax):\n",
    "    '''\n",
    "    Given token ids of a sequence, return the averaged log probability of\n",
    "    unmasked sequence (AULA or AUL).\n",
    "    '''\n",
    "    tokens = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Get the token IDs and attention mask\n",
    "    input_ids = tokens['input_ids']#.to('mps')\n",
    "    output = model(input_ids, output_hidden_states=True)\n",
    "    logits = output.logits.squeeze(0)\n",
    "    log_probs = log_softmax(logits)\n",
    "    input_ids = input_ids.view(-1, 1).detach()\n",
    "    token_log_probs = log_probs.gather(1, input_ids)[1:-1]\n",
    "    sentence_log_prob = torch.mean(token_log_probs)\n",
    "    score = sentence_log_prob.item()\n",
    "\n",
    "    hidden_states = output.hidden_states[-1][:,1:-1].cpu()\n",
    "    hidden_state = torch.mean(hidden_states, 1).detach().numpy()\n",
    "\n",
    "    return score, hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c985b8b3-e6bf-4f0b-8949-94a97a0ad4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "959ccf14-0737-4b95-a639-d5fe4fe7f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = torch.nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e3be07a-2693-4c70-8992-08a3dcff6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "caste_df = read_file(filename=\"../data/Caste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4313ab0-b0b6-4f6b-ac97-2029b7a720f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5efea588-be84-4cbb-89ad-5f86c8c6c5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias score (emb): 62.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stereo_inputs = [i for i in caste_df['Stereotypical']]\n",
    "antistereo_inputs = [i for i in caste_df['Anti-Stereotypical']]\n",
    "\n",
    "stereo_scores = []\n",
    "antistereo_scores = []\n",
    "stereo_embes = []\n",
    "antistereo_embes = []\n",
    "\n",
    "for i in stereo_inputs:\n",
    "    stereo_score, stereo_hidden_state = calculate_aul(model, i, log_softmax)\n",
    "    stereo_scores.append(stereo_score)\n",
    "    stereo_embes.append(stereo_hidden_state)\n",
    "\n",
    "for j in antistereo_inputs:\n",
    "    antistereo_score, antistereo_hidden_state = calculate_aul(model, j, log_softmax)\n",
    "    antistereo_scores.append(antistereo_score)\n",
    "    antistereo_embes.append(antistereo_hidden_state)\n",
    "\n",
    "stereo_scores = np.array(stereo_scores)\n",
    "stereo_scores = stereo_scores.reshape([-1, 1])\n",
    "antistereo_scores = np.array(antistereo_scores)\n",
    "antistereo_scores = antistereo_scores.reshape([1, -1])\n",
    "bias_scores = stereo_scores > antistereo_scores\n",
    "\n",
    "stereo_embes = np.concatenate(stereo_embes)\n",
    "antistereo_embes = np.concatenate(antistereo_embes)\n",
    "weights = cos_sim(stereo_embes, antistereo_embes.T)\n",
    "\n",
    "weighted_bias_scores = bias_scores * weights\n",
    "bias_score = np.sum(weighted_bias_scores) / np.sum(weights)\n",
    "print('bias score (emb):', round(bias_score * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60310dd-9c2c-4aea-b6c0-114dfa197083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4f2e0-f123-4828-a9e1-3609dbf98e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67fc1cb6-2781-45c9-936e-ca434803e2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49ebb71f-3a2d-4ce8-8f48-bf9d7855660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias score (emb): 62.43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stereo_inputs = [i for i in caste_df['Stereotypical']]\n",
    "antistereo_inputs = [i for i in caste_df['Anti-Stereotypical']]\n",
    "\n",
    "stereo_scores = []\n",
    "antistereo_scores = []\n",
    "stereo_embes = []\n",
    "antistereo_embes = []\n",
    "\n",
    "for i in stereo_inputs:\n",
    "    stereo_score, stereo_hidden_state = calculate_aul(model, i, log_softmax)\n",
    "    stereo_scores.append(stereo_score)\n",
    "    stereo_embes.append(stereo_hidden_state)\n",
    "\n",
    "for j in antistereo_inputs:\n",
    "    antistereo_score, antistereo_hidden_state = calculate_aul(model, j, log_softmax)\n",
    "    antistereo_scores.append(antistereo_score)\n",
    "    antistereo_embes.append(antistereo_hidden_state)\n",
    "\n",
    "stereo_scores = np.array(stereo_scores)\n",
    "stereo_scores = stereo_scores.reshape([-1, 1])\n",
    "antistereo_scores = np.array(antistereo_scores)\n",
    "antistereo_scores = antistereo_scores.reshape([1, -1])\n",
    "bias_scores = stereo_scores > antistereo_scores\n",
    "\n",
    "stereo_embes = np.concatenate(stereo_embes)\n",
    "antistereo_embes = np.concatenate(antistereo_embes)\n",
    "weights = cos_sim(stereo_embes, antistereo_embes.T)\n",
    "\n",
    "weighted_bias_scores = bias_scores * weights\n",
    "bias_score = np.sum(weighted_bias_scores) / np.sum(weights)\n",
    "print('bias score (emb):', round(bias_score * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337ac04-66bd-48eb-b7d6-13789aef4358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5916e-86f7-4125-af35-110d212f6140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b311565-fd11-4dd4-92bc-71b3cb60b13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0fb70-a91c-4aa0-b6f3-4b4ebdb11fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
